# -*- coding: utf-8 -*-
"""AppInsights_KQL_Tool.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11EyHEfiiJtR76S7MOPeWgt_DafR7MUto
"""

!pip install langchain tiktoken unstructured openai faiss-cpu

!pip install

from langchain.document_loaders import UnstructuredURLLoader

urls = [
    "https://learn.microsoft.com/en-us/azure/azure-monitor/app/data-model-complete"
]

loader = UnstructuredURLLoader(urls=urls)
data = loader.load()
print(data)

import os
import time
from os import getcwd, listdir
from os.path import isfile, join

from langchain.document_loaders import BSHTMLLoader, DirectoryLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import AzureOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.document_loaders import UnstructuredURLLoader

# Press Shift+F10 to execute it or replace it with your code.
# Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_VERSION"] = "2022-12-01"
os.environ["OPENAI_API_BASE"] = "https://dv3llm01.openai.azure.com/"
os.environ["OPENAI_API_KEY"] = "53e94f3c907a4f9e8042af2494073ed0"

openai_api_key = os.environ["OPENAI_API_KEY"]

urls = [
    "https://learn.microsoft.com/en-us/azure/azure-monitor/app/data-model-complete"
]
def create_index():
    start_time = time.time()
    # cwd = join(getcwd(), "amp_articles")
    # loader = DirectoryLoader(
    #     cwd,
    #     glob="**/*.html",
    #     loader_cls=BSHTMLLoader,
    #     show_progress=True,
    #     use_multithreading=True,
    # )
    UnstructuredURLLoader(urls=urls)
    data = loader.load()
    print(f"loaded {len(data)} documents")
    docs_load_time = time.time()
    print("--- Docs Load time  %s seconds ---" % (docs_load_time - start_time))
    # for file in files:
    #     loader = BSHTMLLoader(file)
    #     data = loader.load()
    #     docs.append(data)
    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=500, chunk_overlap=100, separators=["\n\n", "\n", " ", ""]
    )
    documents = text_splitter.split_documents(data)
    print(f"Splitting into {len(documents)} chunks")
    split_load_time = time.time()
    print(
        "--- Splitting Load time  %s seconds ---" % (split_load_time - docs_load_time)
    )
    embeddings = OpenAIEmbeddings(
        deployment="ada-embeddings", chunk_size="1", model="text-embedding-ada-002"
    )
    db = FAISS.from_documents(documents, embeddings)
    faiss_load_time = time.time()
    print("--- FAISS Load time  %s seconds ---" % (faiss_load_time - split_load_time))
    db.save_local("kqltool_multi_index")
    faiss_save_time = time.time()
    print("--- FAISS saving time  %s seconds ---" % (faiss_save_time - faiss_load_time))
    print("--- Total time  %s seconds ---" % (faiss_save_time - start_time))

create_index()

# using the index created above we are connecting to llm using langchain and asking question
from langchain.prompts.prompt import PromptTemplate
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import AzureOpenAI

import os
import time
from os import getcwd, listdir
from os.path import isfile, join

from langchain.document_loaders import BSHTMLLoader, DirectoryLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import AzureOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.document_loaders import UnstructuredURLLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.chains.llm import LLMChain
from langchain.chains.question_answering import load_qa_chain
from langchain.memory import ConversationBufferMemory
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_VERSION"] = "2022-12-01"
os.environ["OPENAI_API_BASE"] = "https://dv3llm01.openai.azure.com/"
os.environ["OPENAI_API_KEY"] = "53e94f3c907a4f9e8042af2494073ed0"

openai_api_key = os.environ["OPENAI_API_KEY"]
start_time = time.time()
urls = [
    "https://learn.microsoft.com/en-us/azure/azure-monitor/app/data-model-complete"
]

loader = UnstructuredURLLoader(urls=urls)
data = loader.load()
print(f"loaded {len(data)} documents")
docs_load_time = time.time()
print("--- Docs Load time  %s seconds ---" % (docs_load_time - start_time))
# for file in files:
#     loader = BSHTMLLoader(file)
#     data = loader.load()
#     docs.append(data)
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=500, chunk_overlap=100, separators=["\n\n", "\n", " ", ""]
)
documents = text_splitter.split_documents(data)
print(f"Splitting into {len(documents)} chunks")
split_load_time = time.time()
print(
    "--- Splitting Load time  %s seconds ---" % (split_load_time - docs_load_time)
)
embeddings = OpenAIEmbeddings(
    deployment="ada-embeddings", chunk_size="1", model="text-embedding-ada-002"
)
vectorstore = FAISS.from_documents(documents, embeddings)
faiss_load_time = time.time()
print("--- FAISS Load time  %s seconds ---" % (faiss_load_time - split_load_time))
# db.save_local("kqltool_multi_index")
faiss_save_time = time.time()
print("--- FAISS saving time  %s seconds ---" % (faiss_save_time - faiss_load_time))
print("--- Total time  %s seconds ---" % (faiss_save_time - start_time))

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
prompt_template = """

        You are helpful Kusto Query Language Query Generator
        You use the documentation of Telemetry data model written for an application called Application Insights
        and using the telemetry model schema as context provided below. please generate Kusto query for the user asked question at the end

        If you don't know the answer, just say that you don't know, don't try to make up an answer.

        {context}
        Question: {question}
        """

qa_prompt = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)
llm = AzureOpenAI(
        deployment_name="dv3llm01-01",
        model_name="text-embedding-ada-002",
        verbose=True,
        temperature=0,
)

doc_chain = load_qa_chain(
        llm, chain_type="stuff", prompt=qa_prompt
)
qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=memory)


# qa = ConversationalRetrievalQAChain.fromLLM(

# )
# qa = ConversationalRetrievalChain(
#         combine_docs_chain=doc_chain,
#         retriever=db.as_retriever(),
#         return_source_documents=True
# )

qt = """
 get all requests which contains end point of "api"
 """
question = f"Write kusto query to {qt}"
response = qa(
                {"question": question}
)
print(response.get('answer'))

# using the index created above we are connecting to llm using langchain and asking question
from langchain.prompts.prompt import PromptTemplate
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import AzureOpenAI

import os
import time
from os import getcwd, listdir
from os.path import isfile, join

from langchain.document_loaders import BSHTMLLoader, DirectoryLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import AzureOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.document_loaders import UnstructuredURLLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.chains.llm import LLMChain
from langchain.chains.question_answering import load_qa_chain
from langchain.memory import ConversationBufferMemory
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_VERSION"] = "2022-12-01"
os.environ["OPENAI_API_BASE"] = "https://dv3llm01.openai.azure.com/"
os.environ["OPENAI_API_KEY"] = "53e94f3c907a4f9e8042af2494073ed0"

openai_api_key = os.environ["OPENAI_API_KEY"]
start_time = time.time()
urls = [
    "https://learn.microsoft.com/en-us/azure/azure-monitor/app/data-model-complete"
]

loader = UnstructuredURLLoader(urls=urls)
data = loader.load()
print(f"loaded {len(data)} documents")
docs_load_time = time.time()
print("--- Docs Load time  %s seconds ---" % (docs_load_time - start_time))
# for file in files:
#     loader = BSHTMLLoader(file)
#     data = loader.load()
#     docs.append(data)
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=500, chunk_overlap=100, separators=["\n\n", "\n", " ", ""]
)
documents = text_splitter.split_documents(data)
print(f"Splitting into {len(documents)} chunks")
split_load_time = time.time()
print(
    "--- Splitting Load time  %s seconds ---" % (split_load_time - docs_load_time)
)
embeddings = OpenAIEmbeddings(
    deployment="ada-embeddings", chunk_size="1", model="text-embedding-ada-002"
)
vectorstore = FAISS.from_documents(documents, embeddings)
faiss_load_time = time.time()
print("--- FAISS Load time  %s seconds ---" % (faiss_load_time - split_load_time))
# db.save_local("kqltool_multi_index")
faiss_save_time = time.time()
print("--- FAISS saving time  %s seconds ---" % (faiss_save_time - faiss_load_time))
print("--- Total time  %s seconds ---" % (faiss_save_time - start_time))

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
prompt_template = """

        You are helpful Kusto Query Language Query Generator
        You use the documentation of Telemetry data model written for an application called Application Insights
        and using the telemetry model schema as context provided below. please generate Kusto query for the user asked question at the end

        If you don't know the answer, just say that you don't know, don't try to make up an answer.

        {context}
        Question: {question}
        """

qa_prompt = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)
llm = AzureOpenAI(
        deployment_name="dv3llm01-01",
        model_name="text-embedding-ada-002",
        verbose=True,
        temperature=0,
)

doc_chain = load_qa_chain(
        llm, chain_type="stuff", prompt=qa_prompt
)
qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=memory)


# qa = ConversationalRetrievalQAChain.fromLLM(

# )
# qa = ConversationalRetrievalChain(
#         combine_docs_chain=doc_chain,
#         retriever=db.as_retriever(),
#         return_source_documents=True
# )

qt = """
get all requests which contains end point of "api"

 """
question = f"Write kusto query to {qt}"
response = qa(
                {"question": question}
)
print(response.get('answer'))



# using the index created above we are connecting to llm using langchain and asking question
from langchain.prompts.prompt import PromptTemplate
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import AzureOpenAI

import os
import time
from os import getcwd, listdir
from os.path import isfile, join

from langchain.document_loaders import BSHTMLLoader, DirectoryLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import AzureOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.document_loaders import UnstructuredURLLoader
from langchain.chains import ConversationalRetrievalChain
from langchain.chains.llm import LLMChain
from langchain.chains.question_answering import load_qa_chain
from langchain.memory import ConversationBufferMemory
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_VERSION"] = "2022-12-01"
os.environ["OPENAI_API_BASE"] = "https://dv3llm01.openai.azure.com/"
os.environ["OPENAI_API_KEY"] = "53e94f3c907a4f9e8042af2494073ed0"

openai_api_key = os.environ["OPENAI_API_KEY"]
start_time = time.time()
urls = [
    "https://learn.microsoft.com/en-us/azure/azure-monitor/app/data-model-complete"
]

loader = UnstructuredURLLoader(urls=urls)
data = loader.load()
print(f"loaded {len(data)} documents")
docs_load_time = time.time()
print("--- Docs Load time  %s seconds ---" % (docs_load_time - start_time))
# for file in files:
#     loader = BSHTMLLoader(file)
#     data = loader.load()
#     docs.append(data)
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=500, chunk_overlap=100, separators=["\n\n", "\n", " ", ""]
)
documents = text_splitter.split_documents(data)
print(f"Splitting into {len(documents)} chunks")
split_load_time = time.time()
print(
    "--- Splitting Load time  %s seconds ---" % (split_load_time - docs_load_time)
)
embeddings = OpenAIEmbeddings(
    deployment="ada-embeddings", chunk_size="1", model="text-embedding-ada-002"
)
vectorstore = FAISS.from_documents(documents, embeddings)
faiss_load_time = time.time()
print("--- FAISS Load time  %s seconds ---" % (faiss_load_time - split_load_time))
vectorstore.save_local("mkqltool_multi_index")
faiss_save_time = time.time()
print("--- FAISS saving time  %s seconds ---" % (faiss_save_time - faiss_load_time))
print("--- Total time  %s seconds ---" % (faiss_save_time - start_time))

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
prompt_template = """

        You are helpful Kusto Query Language Query Generator
        You use the documentation of Telemetry data model written for an application called Application Insights
        and using the telemetry model schema as context provided below. please generate the proper Kusto queries that display exactly
        what the user was asking for in their prompt.

        If you don't know the answer, just say that you don't know, don't try to make up an answer. Remember, all of your knowledge should be derived
        from the Kusto Query Language documentation page

        {context}
        Question: {question}
        """

qa_prompt = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)
llm = AzureOpenAI(
        deployment_name="dv3llm01-01",
        model_name="text-embedding-ada-002",
        verbose=True,
        temperature=0,
)

doc_chain = load_qa_chain(
        llm, chain_type="stuff", prompt=qa_prompt
)
qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=memory)


# qa = ConversationalRetrievalQAChain.fromLLM(

# )
# qa = ConversationalRetrievalChain(
#         combine_docs_chain=doc_chain,
#         retriever=db.as_retriever(),
#         return_source_documents=True
# )
question = """
Write kusto query to get all requests which contains end point of "api"
 """
response = qa(
                {"question": question}
)
print(response.get('answer'))